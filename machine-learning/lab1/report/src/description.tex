\section{Описание}
В наборе данных имеется категориальный признак, его нужно разделить на несколько признаков с помощью one-hot encoding. Это нужно, чтобы не создавать лишнюю числовую связь. Pandas имеет метод get\_dummies, который это делает.

Для корректной работы моделей признаки нужно нормализовать. scikit-learn позволяет сделать это с помощью normalize \cite{scikit-normalize}, я использую метрику <<max>>.

Теперь данные можно разделить на тестовую и обучающую выборку (80 к 20), используя train\_test\_split из scikit-learn \cite{scikit-split}.

В задании требуется сделать все модели совместимыми с scikit-learn \cite{scikit-develop}, поэтому получение оценок модели \cite{ml-metrics} можно сделать методами из этой же библотеки:
\begin{lstlisting}[language=Python]
def scores(model, X, y_true):
    y_pred = model.predict(X)
    print("Accuracy:", accuracy_score(y_true, y_pred))
    print("Recall:", recall_score(y_true, y_pred))
    print("Precision:", precision_score(y_true, y_pred))
    figure = plt.figure(figsize = (20, 5))
    matr = confusion_matrix(y_true, y_pred)
    ax = plt.subplot(1, 2, 1)
    ConfusionMatrixDisplay(matr).plot(ax = ax)
    ax = plt.subplot(1, 2, 2)
    RocCurveDisplay.from_predictions(y_true = y_true, y_pred = y_pred, name = "ROC-curve", ax = ax)
    plt.show()
\end{lstlisting}

При реализации моделей я использовал шаблон \cite{scikit-github} из scikit-learn, в котором учтены все тонкости реализации: наследование от нужных классов (ClassifierMixin, BaseEstimator) и необходимые методы (fit, predict).
\pagebreak

\subsection{Метод k-ближайших соседей}
Среди всех объектов обучающей выборки ищем k ближайших, среди них классифицируем объект тем классом, которого больше всего среди соседей:
\begin{lstlisting}[language=Python]
class kNN(ClassifierMixin, BaseEstimator):
    def __init__(self, k = 1):
        self.k = k

    def fit(self, X, y):
        # Check that X and y have correct shape
        X, y = check_X_y(X, y)
        # Store the classes seen during fit
        self.classes_ = unique_labels(y)
        self.X_ = X
        self.y_ = y
        # Return the classifier
        return self

    def predict(self, X):
        # Check is fit had been called
        check_is_fitted(self, ['X_', 'y_'])
        # Input validation
        X = check_array(X)
        y = np.ndarray((X.shape[0],))
        for (i, elem) in enumerate(X):
            distances = euclidean_distances([elem], self.X_)[0]
            neighbors = np.argpartition(distances, kth = self. k - 1)
            k_neighbors = neighbors[:self.k]
            labels, cnts = np.unique(self.y_[k_neighbors], return_counts = True)
            y[i] = labels[cnts.argmax()]
        return y
\end{lstlisting}
Использую Евклидову метрику из scikit-learn для вычисления расстояний и метод argpartition из numpy \cite{numpy-argpart} для индексной сортировки.
\pagebreak

\subsection{Логистическая регрессия}
Логистическая регрессия по сути является однослойной нейросетью. Использую наработки прошлых работ, чтобы реализовать модель. Описываю класс сети, которую можно строить из разных слоёв:
\begin{lstlisting}[language=Python]
class Net:
    def __init__(self, loss_function):
        self.layers = []
        self.loss = loss_function()

    def append(self, layer):
        self.layers.append(layer)

    def forward(self, x):
        for layer in self.layers:
            x = layer.forward(x)
        return x

    def backward(self, z):
        for layer in self.layers[::-1]:
            z = layer.backward(z)
        return z

    def forward_loss(self, x, y):
        p = self.forward(x)
        return self.loss.forward(p, y)

    def backward_loss(self, l):
        dp = None
        dp = self.loss.backward(l)
        return self.backward(dp)

    def update(self, step):
        for layer in self.layers:
            if "update" in layer.__dir__():
                layer.update(step)

    def train_epoch(self, x, y, batch_size = 100, step = 1e-7):
        for i in range(0, len(x), batch_size):
            xb = x[i:i + batch_size]
            yb = y[i:i + batch_size]
            loss = self.forward_loss(xb, yb)
            dx = self.backward_loss(loss)
            self.update(step)
\end{lstlisting}
\pagebreak

Линейный слой сети с возможностью обновления весов (изначально матрица весов заполняется случайными числами, а вектор смещения нулями) так же вынесен в отдельный класс:
\begin{lstlisting}[language=Python]
class Linear:
    def __init__(self, n, m):
        mu = 0.0
        sigma = 1.0 / np.sqrt(2.0 * n)
        self.W = np.random.normal(mu, sigma, (m, n))
        self.b = np.zeros((1, m))
        self.dW = np.zeros((m, n))
        self.db = np.zeros((1, m))

    def forward(self, x):
        self.x = x
        z = np.dot(x, self.W.T) + self.b
        return z

    def backward(self, dz):
        dx = np.dot(dz, self.W)
        dW = np.dot(dz.T, self.x)
        db = dz.sum(axis = 0)
        self.dW = dW
        self.db = db
        return dx

    def update(self, step):
        self.W -= step * self.dW
        self.b -= step * self.db
\end{lstlisting}
В логистической регрессии используется функция активации сигмоида, которая так же описана в отдельном классе:
$$\sigma(x) = (1 + e ^ {-x}) ^ {-1}$$
\begin{lstlisting}[language=Python]
class Sigmoid:
    def forward(self, x):
        self.y = 1.0 / (1.0 + np.exp(-x))
        return self.y

    def backward(self, dy):
        return self.y * (1.0 - self.y) * dy
\end{lstlisting}
\pagebreak

В качестве функции потерь использую binary cross entropy loss, так как стоит задача бинарной классификации:
\begin{lstlisting}[language=Python]
class BinaryCrossEntropy:
    def forward(self, p, y):
        y = y.reshape((y.shape[0], 1))
        self.p = p
        self.y = y
        res = y * np.log(p) + (1 - y) * np.log(1 - p)
        return -np.mean(res)

    def backward(self, loss):
        res = (self.p - self.y) / (self.p * (1 - self.p))
        return res / self.p.shape[0]
\end{lstlisting}
Логистическая регрессия содержит нейросеть, состояющую из линейного слоя, сигмиоды и описанной выше функции потерь. Алгоритм обучения сети --- стохастический градиентный спуск с постоянным шагом:
\begin{lstlisting}[language=Python]
class LogisticRegression(ClassifierMixin, BaseEstimator):
    def __init__(self, epoches = 1, batch_size = 10, SGD_step = 0.001):
        self.epoches = epoches
        self.batch_size = batch_size
        self.SGD_step = SGD_step
        self.Net = Net(BinaryCrossEntropy)
        self.Net.append(Linear(8, 1))
        self.Net.append(Sigmoid())

    def fit(self, X, y):
        # Check that X and y have correct shape
        X, y = check_X_y(X, y)
        # Store the classes seen during fit
        self.classes_ = unique_labels(y)
        self.X_ = X
        self.y_ = y
        for _ in range(self.epoches):
            self.Net.train_epoch(X, y, self.batch_size, self.SGD_step)
        # Return the classifier
        return self

    def predict(self, X):
        # Check is fit had been called
        check_is_fitted(self, ['X_', 'y_'])
        # Input validation
        X = check_array(X)
        y = self.Net.forward(X)
        res = np.where(y < 0.5, 0, 1)
        return res
\end{lstlisting}
\pagebreak

\subsection{Метод опорных векторов}
Метод похож на предыдущий, но функция ошибки требует сами данные, на которых происходит обучение, поэтому встроить в класс сети не получилось. Отдельно описываю модель опорных векторов с мягким зазором:
\begin{lstlisting}[language=Python]
class SoftMarginSVM:
    def __init__(self, n, alpha):
        self.alpha = alpha
        mu = 0.0
        sigma = 1.0 / np.sqrt(n)
        self.W = np.random.normal(mu, sigma, (1, n + 1))

    def forward(self, x):
        z = np.dot(x, self.W.T)
        return z

    def add_ones(self, x):
        ones = np.ones((x.shape[0], 1))
        return np.hstack((x, ones))

    def predict(self, x):
        res = self.forward(self.add_ones(x))
        return np.where(res < 0, 0, 1)

    def train_epoch(self, x, y, batch_size = 100, step = 1e-7):
        x = self.add_ones(x)
        y = np.where(y > 0, 1, -1)
        for i in range(0, len(x), batch_size):
            xb = x[i:i + batch_size]
            yb = y[i:i + batch_size]

            pred = self.forward(xb)
            grad = self.alpha * self.W
            for i in range(len(xb)):
                if (yb[i] * pred[i] < 1):
                    grad -= yb[i] * xb[i]
            self.W -= step * grad
\end{lstlisting}
Класс получился простой, но не очень универсальный. Чтобы отбросить вектор смещения $b$, я ввёл искуственный признак, который всегда равен единице \cite{habr-svm}.
\pagebreak

Эта модель затем встраивается в классификатор. Параметры почти такие же, как и в случае с логистической регрессией:
\begin{lstlisting}[language=Python]
class SVM(ClassifierMixin, BaseEstimator):
    def __init__(self, epoches = 1, batch_size = 10, SGD_step = 0.001, alpha = 0.1):
        self.epoches = epoches
        self.batch_size = batch_size
        self.SGD_step = SGD_step
        self.alpha = alpha
        self.Net = SoftMarginSVM(8, alpha)

    def fit(self, X, y):
        # Check that X and y have correct shape
        X, y = check_X_y(X, y)
        # Store the classes seen during fit
        self.classes_ = unique_labels(y)

        self.X_ = X
        self.y_ = y
        for _ in range(self.epoches):
            self.Net.train_epoch(X, y, self.batch_size, self.SGD_step)
        # Return the classifier
        return self

    def predict(self, X):
        y = self.Net.predict(X)
        return y
\end{lstlisting}
\pagebreak

\subsection{Наивный байесовский классификатор}
Идея модели в наивном предположении о независимости параметров. Так же часто используется модель с нормальным распределением признаков. В прошлой работе гистограммы $4$ из $6$ распределены нормально, поэтому попробую применить такой метод:
\begin{lstlisting}[language=Python]
class NaiveBayes(ClassifierMixin, BaseEstimator):
    def __init__(self):
        None

    def fit(self, X, y):
        # Check that X and y have correct shape
        X, y = check_X_y(X, y)

        self.X_ = X
        self.y_ = y

        labels, cnts = np.unique(self.y_, return_counts = True)
        self.labels = labels
        self.p_of_y = np.array([elem / self.y_.shape[0] for elem in cnts])
        self.means = np.array([self.X_[self.y_ == elem].mean(axis = 0) for elem in labels])
        self.stds = np.array([self.X_[self.y_ == elem].std(axis = 0) for elem in labels])
        # Return the classifier
        return self

    def gaussian(self, mu, sigma, x0):
        return np.exp(-(x0 - mu) ** 2 / (2 * sigma)) / np.sqrt(2.0 * pi * sigma)

    def predict(self, X):
        # Check is fit had been called
        check_is_fitted(self, ['X_', 'y_'])

        # Input validation
        X = check_array(X)

        res = np.zeros(X.shape[0])
        for (i, elem) in enumerate(X):
            p = np.array(self.p_of_y)
            for (j, label) in enumerate(self.labels):
                p_x_cond_y = np.array([self.gaussian(self.means[j][k], self.stds[j][k], elem[k]) for k in range(X.shape[1])])
                p[j] *= np.prod(p_x_cond_y)
            res[i] = np.argmax(p)
        return res
\end{lstlisting}
\pagebreak

\subsection{Подбор гиперпараметров}
Для подбора гиперпараметров используются кросс-валидации GridSearchCV \cite{scikit-grid} и RandomizedSearchCV \cite{scikit-rand}. Приведу пример использования с SVM:
\begin{lstlisting}[language=Python]
gscv = GridSearchCV(Pipeline([("SVM", SVM())]),
                    {"SVM__epoches" : [1, 2, 4],
                     "SVM__batch_size" : [5, 10, 20],
                     "SVM__SGD_step" : [0.01, 0.05, 0.1],
                     "SVM__alpha" : [1.0, 0.1, 0.01, 0.0]})
gscv.fit(train_X, train_y)
best(gscv)
\end{lstlisting}
\begin{lstlisting}[language=Python]
rscv = RandomizedSearchCV(Pipeline([("SVM", SVM())]),
                    {"SVM__epoches" : [1, 2, 4],
                     "SVM__batch_size" : [5, 10, 20],
                     "SVM__SGD_step" : [0.01, 0.05, 0.1],
                     "SVM__alpha" : [1.0, 0.1, 0.01, 0.0]})
rscv.fit(train_X, train_y)
best(rscv)
\end{lstlisting}

Наивный байесовский классификатор не имеет параметров, поэтому для него поиск гиперпараметров не осуществляется.
\pagebreak
